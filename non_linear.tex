\documentclass{article}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{mathtools}
\DeclarePairedDelimiter\abs{\lvert}{\rvert}
\DeclarePairedDelimiter\norm{\lVert}{\rVert}
\DeclarePairedDelimiter\inner{\langle}{\rangle}
\def\P{\mathcal{P}}
\def\E{\mathbb{E}}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclareMathOperator*{\diag}{diag}
\DeclareMathOperator*{\argmin}{argmin}
\newtheorem{lemma}{Lemma}
\title{Non-linearity in random variable approximation}
\author{Feng Zhao}
\begin{document}
\maketitle
Let $Y, X_1, \dots, X_k$ be n-dimensional uniformly distributed random vector on unit sphere. $X=(X_1, \dots, X_k)$ is an $n\times k$ random matrix. $Y$ is independent with $X$. Each sample $x_1, \dots, x_k$ from $X$ have the property that $x_i \cdot x_j = 0$. We would like to compute the following quantity:
\begin{equation}\label{eq:Eminw}
\E[\min_w \norm{Y - \sigma (X w ) }^2]
\end{equation}
$\sigma$ is generally a non-linear scalar function. Its application on a vector is element-wise.
\begin{lemma}\label{lem:uniform}
Suppose $X$ is an $n$ by $k$ random matrix. Each sample $x_1, \dots, x_k$ from $X$ have the property that $x_i \cdot x_j = 0, \norm{x_i}=1$ where $x_i$ is the $i$-th column.  Let $A=X X^T$, then we have
\begin{equation}
E[A_{ij}]= \begin{cases}
\frac{k}{n} & i = j\\
0 & i\neq j 
\end{cases}
\end{equation}
\end{lemma}
\begin{proof}
We can use a generator model to simulate the sampling of $X$. First we random select $x_1$ from uniform distributed random variable on an $n$ dimensional sphere. Then $x_2$ should be selected from the $n-1$ dimensional sphere orthogonal to $x_1$ and so on.
It is easier to show $\E[X_1X_1^T] = \frac{1}{n} I_n$ since we can write $X_1 = \frac{(a_1, a_2, \dots a_n) }{\sqrt{a_1^2+\dots + a_n^2}}$ where $a_1, \dots a_n$ are i.i.d Gaussian.

To show $\E[A]=\frac{k}{n}I_n$ we need to show respectively $\E[X_iX_i^T]=\frac{1}{n} I_n$. We first show this equality holds for $i=2$. From the generator model, $X_2$ depends on $X_1$. By the Law of total expectation we have $\E[X_2 X_2^T] = \E_{X_1}[\E[X_2 X_2^T |X_1 = x_1]]$. $X_2 | x_1$ is a random variable distributed on an $n-1$ sphere. We assume $b_1, \dots, b_{n-1}$ is an orthogonal unit basis for this $n-1$ dimensional space,
then $b_1, \dots, b_{n-1}, x_1$ are an orthogonal unit basis for the $n$ dimensional space. Therefore we have
$\sum_{i=1}^{n-1} b_i b_i^T = I_n -  x_1 x_1^T $. The RHS is fixed for given $x_1$, which is irrelevant with the choice of $b_1, \dots, b_{n-1}$. We can show that the inner expectation $\E[X_2 X_2^T |X_1] = \frac{1}{n-1}(I_n - x_1 x_1^T)$ since we can choose $X_2 = \frac{1}{\sqrt{a_1^2 + \dots + a_{n-1}^2}} \sum_{i=1}^{n-1} a_i b_i$ ($b_i$ is fixed vector while $a_i$ is scalar random vector). Then taking the outer expectation we have $\E[X_2 X_2^T] = \frac{1}{n-1} (I_n - \frac{1}{n} I_n) = \frac{1}{n} I_n$.

For $i>2$, the proof is similar as we have $$
\E[X_i X_i^T] = \E_{X_1, \dots, X_{i-1}} [\E[X_i X_i^T | x_1, \dots x_{i-1}]]
$$
 which equals $\frac{1}{n-i+1}(I_n - \frac{i-1}{n} I_n) = \frac{1}{n} I_n$.
\end{proof}
\begin{lemma}\label{lem:x2y2}
Suppose $(X,Y)$ is two-dimensional Gaussian vector, has zero mean and covariance vector $\Sigma$, then 
\begin{align}
\E[X^2 Y^2] &= \Sigma_{11}\Sigma_{22} + 2\Sigma_{12}^2 \\
\E[X^3 Y + Y^3 X ] &= 3 (\Sigma_{11} + \Sigma_{22}) \Sigma_{12} \\
\E[X^3 Y^3] & = 6 \Sigma_{12}^3 + 9 \Sigma_{12} \Sigma_{11} \Sigma_{22}
\end{align}
\end{lemma}
\begin{proof}
Let $\Sigma = L^T L $ and $\binom{x}{y} = L^T \binom{x'}{y'}$, then $\binom{x'}{y'}$ has identity covariance. By Cholesky decomposition we can make $L$ be upper trianglar matrix (
$L_{12}=0$) with $L_{11}^2 = \Sigma_{11}, L_{11}L_{12} = \Sigma_{12}, L_{22}^2 = \Sigma_{22} - \frac{\Sigma_{12}^2}{\Sigma_{11}}$. Then we have $x = L_{11} x', y = L_{12} x' + L_{22} y'$. Therefore
\begin{align*}
\E[X^2 Y^2] & = L_{11}^2 (L_{12}^2\E[X'^4]+ L^2_{22}\E[X'^2]\E[Y'^2]) \\
& = L_{11}^2(3L_{12}^2 + L^2_{22}) \\
& = \Sigma_{11}\Sigma_{22} + 2\Sigma_{12}^2
\end{align*}
\end{proof}
\begin{lemma}\label{lem:abcd}
Suppose $X_1, X_2, Y_1, Y_2$ are gaussian random variables and the covariance of their join distribution is block diagonal:
$(X_1, X_2, Y_1, Y_2) \sim N(0, \binom{\Sigma_1, 0}{0, \Sigma_2})$. Then the following equality holds:
\begin{equation}
\E[f(X_1X_2)g(Y_1Y_2)] = \E[f(X_1 X_2)] \E[g(Y_1 Y_2)]
\end{equation}
\end{lemma}
\begin{proof}
\begin{align*}
\E[f(X_1X_2)g(Y_1Y_2)] & = \int \frac{f(x_1 x_2) g(y_1 y_2) }{(2\pi)^2 \sqrt{\abs{\Sigma_1}\abs{\Sigma_2}}}\exp(-\frac{1}{2}(x_1, x_2) \Sigma_1^{-1} \binom{x_1}{x_2}-\frac{1}{2}(y_1, y_2) \Sigma_1^{-1} \binom{y_1}{y_2})dx_1 d x_2 dy_1 dy_2 \\
& = \int \frac{f(x_1 x_2)  }{(2\pi) \sqrt{\abs{\Sigma_1}}}\exp(-\frac{1}{2}(x_1, x_2) \Sigma_1^{-1} \binom{x_1}{x_2})dx_1 d x_2  \\
&\cdot  \int \frac{ g(y_1 y_2)  }{(2\pi) \sqrt{\abs{\Sigma_1}}}\exp(-\frac{1}{2}(y_1, y_2) \Sigma_1^{-1} \binom{y_1}{y_2})dy_1 d y_2 \\
& = \E[f(X_1 X_2)] \E[g(Y_1 Y_2)]
\end{align*}
\end{proof}
We assume $\sigma(z) = z + \epsilon \xi(z)$. When $\epsilon = 0$, the optimal $w$ for given $X, Y$ is 
$\bar{w} = X^T Y $. For small $\epsilon$, we assume $ w = \bar{w} + \epsilon \hat{w} + \epsilon^2 \tilde{w}$. Then we can expand $\norm{Y - \sigma (X w ) }^2$ as follows:
\begin{align*}
\norm{Y - \sigma (X w ) }^2 & = \norm{Y - XX^T Y - \epsilon X \hat{w} - \epsilon^2 X \tilde{w} - \epsilon \xi(XX^T Y + \epsilon X\hat{w}) }^2 \\
& = \norm{ Y - XX^T Y - \epsilon (X\hat{w} + \xi(XX^T Y)) - \epsilon^2(X\tilde{w} + \nabla \xi(XX^TY)X\hat{w})}^2 \\
& = \norm{Y-XX^T Y }^2 - 2 \epsilon (X \hat{w} + \xi(XX^TY))^T (Y-XX^TY) +\\
& +\epsilon^2(\norm{X\hat{w} + \xi(XX^TY)}^2 - 2 (X \tilde{w}+ \nabla \xi(XX^TY)X\hat{w})^T(Y-XX^TY))
\end{align*}
In the above formula,  we expand $\xi$ at $XX^TY$. The Jacobi is actually a diagonal matrix whose $i$-th entry is $\xi'([XX^TY]_i)$. $[\cdot]_i$ represents the $i$-th element of a vector.
We first notice that for given $X$, $XX^TY$ is the projection of $Y$ onto linear subspace spanned by columns of $X$. We use $\tilde{Y}$ to denote the mirror of $Y$ about this linear subspace. Then we have
$XX^T Y = XX^T \tilde{Y}$ and $(Y- XX^TY) = -(\tilde{Y} - XX^T \tilde{Y})$. By symmetric property, the integration of coefficient of $\epsilon$ with respect to $Y$ is zero (given $X$). For the coefficient of $\epsilon^2$ we have
\begin{equation*}
\textrm{Coeff}(\epsilon^2)  =  \E[\norm{X\hat{w} + \xi(XX^TY)}^2 - 2 (\nabla \xi(XX^TY)X\hat{w})^T(Y-XX^TY)]
\end{equation*}
We have written Equation \eqref{eq:Eminw} in $ \norm{Y - X X^T Y}^2 + \textrm{Coeff}(\epsilon^2) \epsilon^2 + o(\epsilon^2)$.

Minimizing $\E[\textrm{Coeff}(\epsilon^2)]$ we have
\begin{equation}
\min \textrm{Coeff}(\epsilon^2) = \E[\norm{\xi(XX^TY)}^2 - \norm{X^T\xi(XX^TY)}^2 - \norm{X^T \nabla\xi(XX^TY)(Y-XX^TY)}^2]
\end{equation}
Below we assume $X$ is given (the expectation is taken first about $Y$ and then about $X$), the columns of $X$ are orthogonal by the given condition ($X^TX=I_k$) and $Y_1, \dots, Y_n $ be component variable of $Y$. The covariance matrix of $Y$ is $\frac{1}{n}I_n$ from Lemma \ref{lem:uniform}. We use $I_1, I_2, I_3$ to denote the three terms of $\min \textrm{Coeff}(\epsilon^2)$ with given $X$ and $A=XX^T$ is an $n\times n$ matrix. That is $ \min \textrm{Coeff}(\epsilon^2) = \E_{X} [I_1 + I_2 + I_3]$.
Let $Z = AY$. Then the covariance matrix of $Z$ is $\frac{AA^T}{n} = \frac{A}{n}$.
Also, both $Y$ and $Z$ have zero mean (vector). We then have
\begin{equation*}
I_1 = \sum_{i=1}^n \int_z \xi^2(z) p_i(z) dz
\end{equation*}
where $Z_i$ has the distribution $p_i$.
 
For $I_2$
we have
\begin{equation*}
-I_2   = \sum_{i,j=1, i \neq j}^n A_{ij} \iint_{x,y} \xi(x)\xi(y) p_{ij}(x,y) dx dy + \sum_{i=1}^n A_{ii} \int_z \xi^2(z) p_i(z) dz 
\end{equation*}
where $Z_i, Z_j$ have the joint distribution $p_{ij}$.

Since we have $XX^T + \bar{X}\bar{X}^T = I_n$, $A_{ii} = \sum_{j=1}^k x^2_{ij} < 1$. Then we have
\begin{equation}\label{eq:I1plusI2}
I_1+ I_2 =   \sum_{i=1}^n (1-A_{ii}) \int_z \xi^2(z) p_i(z) dz  - \sum_{i,j=1, i \neq j}^n A_{ij} \iint_{x,y} \xi(x)\xi(y) p_{ij}(x,y) dx dy
\end{equation}
For $I_3$, the formula is quite tricky. We assume $\xi(z) = z^2 + a z + b $, then $\nabla \xi (AY) = \diag\{2AY+ a\}$. 
Then
\begin{align*}
-I_3 & = \E_Y[\norm{X^T \nabla\xi(XX^TY)(Y-XX^TY)}^2] \\ 
 & = \sum_{i,j=1, i \neq j}^n A_{ij}\Sigma_{ij} + \sum_{i=1}^n A_{ii}\Sigma_{ii} 
\end{align*}
For $\Sigma_{ii}$ we have
\begin{equation}\label{eq:sigmaii}
\Sigma_{ii} = \int_y [(I - A)Y]^2_i (2[AY]_i + a)^2 p(y)dy
\end{equation}
The above integral is about $y_1, \dots, y_n$, which is a quadratic function about $a$.
For $\Sigma_{ij}$ with $i\neq j$ we have
\begin{equation*}
\Sigma_{ij} = \int_y [(I - A)Y]_i (2[AY]_i + a)[(I - A)Y]_j (2[AY]_j + a) p(y)dy
\end{equation*}
which is also a quadratic function about $a$.
For $I_1 + I_2$, the coefficient of $ab$ is zero since the expectation of $Z$ is zero. The other terms are all quandratic about $a$ and $b$ respectively. Therefore, we can find optimal $a$ and $b$ such that $I_1 + I_2 + I_3$ takes the maximum value.

The coefficient of $b^2, b$ of $I_1 + I_2$ takes the following form respectively ($I_3$ is irrelevant with $b$):
\begin{align*}
\textrm{Coeff}(b^2) & = \sum_{i=1}^n (1-A_{ii}) - \sum_{i,j=1, i\neq j} A_{ij} \\
\textrm{Coeff}(b) & = \frac{2}{n}\sum_{i=1}^n A_{ii} [ 1 - A_{ii} - \sum_{j=1, j\neq i} A_{ij} ]
\end{align*}
The expectation about $\textrm{Coeff}(b^2)$ is positive. From Lemma \ref{lem:uniform}
$$
\E[A_{ij}]= \begin{cases}
\frac{k}{n} & i = j\\
0 & i\neq j 
\end{cases}
$$
Then $\E_X[\textrm{Coeff}(b^2)]  = n-k > 0$. 
Therefore we can take appropriate $b$ such that $\min \textrm{Coeff}(\epsilon^2)$ is positive.
To compute the coefficient of $a^2$, we let $\widetilde{Z}=(I-A)Y$, which has zero mean and covariance matrix $(I-A)(I-A)^T = \frac{I-A}{n}$.  We use $\textrm{Coeff}(a^2 | \Sigma_{ij})$ to represent the coefficient of $a^2$ for expression $\Sigma_{ij}$. Then  we have
\begin{align*}
\textrm{Coeff}(a^2 | \Sigma_{ii}) & = \frac{1-A_{ii}}{n} \\
\textrm{Coeff}(a^2 | \Sigma_{ij}) & = \frac{-A_{ij}}{n}  \textrm{ for } i \neq j
\end{align*}
\begin{align*}
\textrm{Coeff}(a^2) & = \frac{1}{n}\sum_{i=1}^n (1-A_{ii})A_{ii}  - \sum_{i,j=1, i\neq j}^n \frac{A_{ij}^2}{n} + \sum_{i,j=1, i \neq j}^n \frac{A^2_{ij}}{n} - \frac{1}{n}\sum_{i=1}^n (1-A_{ii})A_{ii} \\
& = 0\\
\end{align*}
The expectation of $\textrm{Coeff}(a^2)$ is 0.

We conclude that the sign of $\min \textrm{Coeff}(\epsilon^2)$ is undetermined in the case of quadratic linear function pertubation.

To compute the coefficient of $a$, we can show that $\E[Y^3]=0$  and $\E[Z^3]=0$ and further show that the coefficient is zero.

To compute the coefficient of the constant term, we further assume $Y \sim N(0, \frac{1}{n}I_n)$, then $Z \sim N(0, \frac{1}{n}A)$. From Equation \eqref{eq:I1plusI2} and Lemma \ref{lem:x2y2} The contribution of $I_1+ I_2$ to the constant term is 
$ \sum_{i=1}^ n \frac{3}{n^2} A_{ii}^2 (1-A_{ii}) - \frac{1}{n^2} \sum_{i,j=1, i\neq j}^n A_{ij}(A_{ii}A_{jj}+2A_{ij}^2)$.

To compute the constant term in Equation \eqref{eq:sigmaii}, we have $\Sigma_{ii} = 4\E[Z_i^2 (Y_i-Z_i)^2] = \frac{4}{n^2} A_{ii}(1-A_{ii})$;
Using Lemma \ref{lem:abcd} and $(Z_i, Z_j)$ are independent with $(Y_i - Z_i, Y_j - Y_j)$ we have $\Sigma_{ij} = -\frac{4}{n^2} A_{ij}^2$ for $i \neq j$. Combining the contribution from $I_3$ we have:
\begin{equation}
\textrm{Coeff(Const)} = \frac{7}{n^2} \sum_{i=1}^n A^2_{ii} (1 -A_{ii}) - \frac{1}{n^2} \sum_{i,j=1,i\neq j}^n (A_{ij}A_{ii}A_{jj} + 6 A_{ij}^3)
\end{equation}
Now we assume $\xi(z) = \alpha z^2 + \beta z + \gamma$, fixing $A$ we can get $I_1 + I_2 + I_3$ is a quadratic form of $p^T = (\alpha, \beta, \gamma)$:
$$
I_1 + I_2 + I_3 =p^T \begin{pmatrix} 
\textrm{Coeff(Const)}  & 0 & \textrm{Coeff}(b) \\ 
0  & 0 & 0 \\ 
\textrm{Coeff}(b)   & 0 & \textrm{Coeff}(b^2) 
\end{pmatrix} p
$$
Therefore, the result is irrelevant with the coefficient of $\beta$.  We can simplify the above formular as:
\begin{equation}\label{eq:simI3}
I_1+I_2+I_3 = p^T M p \textrm{ with } p^T = (\alpha, \gamma), M = \begin{pmatrix} \textrm{Coeff(Const)}  &  \textrm{Coeff}(b) \\ \textrm{Coeff}(b) &  \textrm{Coeff}(b^2)  \end{pmatrix}
\end{equation}

The expectation of $\E[A_{ii} A_{ij}]$ for $i \neq j $ is zero since if $X= [x_1, \dots, x_k]$ is a bunch of orthogonal basis, $-X$ is also a basis set. Therefore, if we replace $X$ with $-X$ the integral is unchanged. That is $\E[A_{ii} A_{ij}] = -\E[A_{ii}A_{ij}]$ since $A_{ij}$ is odd. 

Therefore, the expectation of $M$ in Equation \ref{eq:simI3} is:
\begin{equation}\label{eq:2t2}
\E[M] = \begin{pmatrix} \frac{7}{n^2} \sum_{i=1}^n (\E[A_{ii}^2 ]-\E[A_{ii}^3]) & \frac{2}{n} \sum_{i=1}^n (\E[A_{ii}]-\E[A^2_{ii}]) \\
\frac{2}{n} \sum_{i=1}^n (\E[A_{ii}]-\E[A^2_{ii}]) & \sum_{i=1}^n (1-\E[A_{ii}])
 \end{pmatrix}
\end{equation}
Let $X^T=[\hat{x}_1, \dots, \hat{x}_n]$, $A_{ii}= \hat{x}_i^T \hat{x}_i$. We assume that $\E[x_{i1}^2] = \E[x_{11}^2]$ and for very large $n$ $X_{11} \sim N(0,\frac{1}{n})$. Therefore
approximation can be made about $\E[A_{ii}^2]$ and 
$\E[A_{ii}^3]$, which equals $\frac{k^2}{n^2}$ and $\frac{k^3}{n^3}$ respectively. Let $ r = \frac{k}{n}$, 
$\E[M] $ can be approximated by 
$$
\begin{pmatrix}
\frac{7}{n}(r^2 - r^3) & 2(r-r^2) \\
2(r-r^2) & n(1-r)
\end{pmatrix}
$$
the determinant of $\E[M]$ is equal to $3r^2(1-r^2) > 0 $. Therefore, $\E[M]$ is positive definite matrix.

Now we consider $ \xi(z) = z^3 + cz^2 + az + b$, then $\nabla \xi (AY) $ is a diagonal matrix whose $(i,i)$ entry is 
 $[\nabla \xi (AY)]_{i,i} = 3 [AY]_i^2 + 2c [AY]_i + a $, then $\Sigma_{ii}$ for $I_3$ has the following form:
\begin{align*}
\Sigma_{ii} & = \E[ [(I-A)Y]_i^2 [\nabla \xi(AY)]_{i,i}^2] \\
& =  \frac{6a}{n^2} A_{ii}(1-A_{ii}) + \frac{27}{n^3} (1-A_{ii})A_{ii}^2 + \dots
\end{align*}
Similarly, we have
\begin{align*}
\Sigma_{ij} & = \E[ [(I-A)Y]_i [(I-A)Y]_j [\nabla \xi(AY)]_{i,i} [\nabla \xi(AY)]_{j,j}] \\
& = -\frac{3a}{n^2} A_{ij}(A_{ii}+A_{jj}) - \frac{9}{n^3}(A_{ij}A_{ii} A_{jj} + 2 A_{ij}^3) + \dots
\end{align*}
Therefore, $I_3$ has the following form:
\begin{align*}
I_3 & = - \sum_{i=1}^n A_{ii} \Sigma_{ii} - \sum_{i,j=1, i\neq j}^n A_{ij} \Sigma_{ij} \\
& = \textrm{Coeff}(a | I_3, \xi) a  + \textrm{Coeff}( \textrm{Const} | I_3, \xi) + \dots
\end{align*}
We can show that $\textrm{Coeff}(a | I_1 + I_2, \xi) =- \textrm{Coeff}(a | I_3, \xi)$. Therefore, $\textrm{Coeff}(a | I_1 + I_2 + I_3, \xi) = 0$.
For the constant coefficient, we have
\begin{equation*}
\textrm{Coeff}(\textrm{Const} | I_1 + I_2, \xi) = \frac{15}{n^3}\sum_{i=1}^n (1-A_{ii}) A_{ii}^3 - \frac{1}{n^3}\sum_{i,j=1,i\neq j}^n (9 A^2_{ij} A_{ii}A_{jj} + 6 A_{ij}^4)
\end{equation*}
Summing with $\textrm{Coeff}(\textrm{Const} | I_3, \xi) $ we have:
\begin{equation}\label{eq:c3}
\textrm{Coeff}(\textrm{Const} | \xi) = \frac{12}{n^3} \sum_{i=1}^n (\sum_{j=1}^n A_{ij}^4 - A_{ii}^3)
\end{equation}
For third order $\xi(z)$, the coefficient of $\epsilon^2$ can be written as the quandratic form of $(c,a,b)$ plus a constant term. We can show that the quandratic form is irrelevant with $a$ and the remaining $2\times 2 $ matrix is equal to the matrix in Equation \eqref{eq:2t2}, which is positive definite. The constant term is shown in Equation \eqref{eq:c3}.

$\E[A_{ij}^4]$ can be approximated by $\frac{k^2}{n^4}$. Therefore $\textrm{Coeff}(\textrm{Const} | \xi) < 0 $ for sufficient large $n$. Therefore we can take $\xi(z) = z^3$ which performs better than linear function approximation.
\end{document}
