\documentclass{article}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{mathtools}
\DeclarePairedDelimiter\abs{\lvert}{\rvert}
\DeclarePairedDelimiter\norm{\lVert}{\rVert}
\DeclarePairedDelimiter\inner{\langle}{\rangle}
\def\P{\mathcal{P}}
\def\E{\mathbb{E}}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclareMathOperator*{\diag}{diag}
\DeclareMathOperator*{\argmin}{argmin}
\newtheorem{lemma}{Lemma}
\title{Non-linearity in random variable approximation}
\author{Feng Zhao}
\begin{document}
\maketitle
Let $Y, X_1, \dots, X_k$ be n-dimensional uniformly distributed random vector on unit sphere. $X=(X_1, \dots, X_k)$ is an $n\times k$ random matrix. $Y$ is independent with $X$. Each sample $x_1, \dots, x_k$from $X$ have the property that $x_i \cdot x_j = 0$. We would like to compute the following quantity:
\begin{equation}\label{eq:Eminw}
\E[\min_w \norm{Y - \sigma (X w ) }^2]
\end{equation}
$\sigma$ is generally a non-linear scalar function. Its application on a vector is element-wise.
\begin{lemma}
Suppose $X$ is an $n$ by $k$ random matrix. Each sample $x_1, \dots, x_k$ from $X$ have the property that $x_i \cdot x_j = 0, \norm{x_i}=1$ where $x_i$ is the $i$-th column.  Let $A=X X^T$, then we have
\begin{equation}
E[A_{ij}]= \begin{cases}
\frac{k}{n} & i = j\\
0 & i\neq j 
\end{cases}
\end{equation}
\end{lemma}
\begin{proof}
We can use a generator model to simulate the sampling of $X$. First we random select $x_1$ from uniform distributed random variable on an $n$ dimensional sphere. Then $x_2$ should be selected from the $n-1$ dimensional sphere orthogonal to $x_1$ and so on.
It is easier to show $\E[X_1X_1^T] = \frac{1}{n} I_n$ since we can write $X_1 = \frac{(a_1, a_2, \dots a_n) }{\sqrt{a_1^2+\dots + a_n^2}}$ where $a_1, \dots a_n$ are i.i.d Gaussian.

To show $\E[A]=\frac{k}{n}I_n$ we need to show respectively $\E[X_iX_i^T]=\frac{1}{n} I_n$. We first show this equality holds for $i=2$. From the generator model, $X_2$ depends on $X_1$. By the Law of total expectation we have $\E[X_2 X_2^T] = \E_{X_1}[\E[X_2 X_2^T |X_1 = x_1]]$. $X_2 | x_1$ is a random variable distributed on an $n-1$ sphere. We assume $b_1, \dots, b_{n-1}$ is an orthogonal unit basis for this $n-1$ dimensional space,
then $b_1, \dots, b_{n-1}, x_1$ are an orthogonal unit basis for the $n$ dimensional space. Therefore we have
$\sum_{i=1}^{n-1} b_i b_i^T = I_n -  x_1 x_1^T $. The RHS is fixed for given $x_1$, which is irrelevant with the choice of $b_1, \dots, b_{n-1}$. We can show that the inner expectation $\E[X_2 X_2^T |X_1] = \frac{1}{n-1}(I_n - x_1 x_1^T)$ since we can choose $X_2 = \frac{1}{\sqrt{a_1^2 + \dots + a_{n-1}^2}} \sum_{i=1}^{n-1} a_i b_i$ ($b_i$ is fixed vector while $a_i$ is scalar random vector). Then taking the outer expectation we have $\E[X_2 X_2^T] = \frac{1}{n-1} (I_n - \frac{1}{n} I_n) = \frac{1}{n} I_n$.

For $i>2$, the proof is the similar as we have $$
\E[X_i X_i^T] = \E_{X_1, \dots, X_{i-1}} [\E[X_i X_i^T | x_1, \dots x_{i-1}]]
$$
 which equals $\frac{1}{n-i+1}(I_n - \frac{i-1}{n} I_n) = \frac{1}{n} I_n$.
\end{proof}
We assume $\sigma(z) = z + \epsilon \xi(z)$. When $\epsilon = 0$, the optimal $w$ for given $X, Y$ is 
$\bar{w} = X^T Y $. For small $\epsilon$, we assume $ w = \bar{w} + \epsilon \hat{w} + \epsilon^2 \tilde{w}$. Then we can expand $\norm{Y - \sigma (X w ) }^2$ as follows:
\begin{align*}
\norm{Y - \sigma (X w ) }^2 & = \norm{Y - XX^T Y - \epsilon X \hat{w} - \epsilon^2 X \tilde{w} - \epsilon \xi(XX^T Y + \epsilon X\hat{w}) }^2 \\
& = \norm{ Y - XX^T Y - \epsilon (X\hat{w} + \xi(XX^T Y)) - \epsilon^2(X\tilde{w} + X\hat{w} + \nabla \xi(XX^TY)X\hat{w})}^2 \\
& = \norm{Y-XX^T Y }^2 - 2 \epsilon (X \hat{w} + \xi(XX^TY))^T (Y-XX^TY) +\\
& +\epsilon^2(\norm{X\hat{w} + \xi(XX^TY)}^2 - 2 (X\hat{w} + X \tilde{w}+ \nabla \xi(XX^TY)X\hat{w})^T(Y-XX^TY))
\end{align*}
We first notice that for given $X$, $XX^TY$ is the projection of $Y$ onto linear subspace spanned by columns of $X$. We use $\tilde{Y}$ to denote the mirror of $Y$ about this linear subspace. Then we have
$XX^T Y = XX^T \tilde{Y}$ and $(Y- XX^TY) = -(\tilde{Y} - XX^T \tilde{Y})$. By symmetric property, the integration of coefficient of $\epsilon$ with respect to $Y|X$ is zero. For the coefficent of $\epsilon^2$ we have
\begin{equation*}
\textrm{Coeff}(\epsilon^2)  =  \norm{X\hat{w} + \xi(XX^TY)}^2 - 2 (\nabla \xi(XX^TY)X\hat{w})^T(Y-XX^TY)
\end{equation*}
We have written Equation \eqref{eq:Eminw} in $ \norm{Y - X X^T Y}^2 + \E[\textrm{Coeff}(\epsilon^2)] \epsilon^2 + o(\epsilon^2)$.

Minimizing $\E[\textrm{Coeff}(\epsilon^2)]$ we have
\begin{equation}
\min \textrm{Coeff}(\epsilon^2) = \E[\norm{\xi(XX^TY)}^2 - \norm{X^T\xi(XX^TY)}^2 - \norm{X^T \nabla\xi(XX^TY)(Y-XX^TY)}^2]
\end{equation}
Below we assume $X$ is given (the expectation is taken first about $Y$ and then about $X$), the columns of $X$ are orthogonal by the given condition ($X^TX=I_k$) and $Y_1, \dots, Y_n  \sim N(0, \frac{1}{n})$ and they are independent. We use $I_1, I_2, I_3$ to denote the three terms of $\min \textrm{Coeff}(\epsilon^2)$ with given $X$ and $A=XX^T$ is an $n\times n$ matrix. Let $Z = AY$. Then $Z \sim N(0, \frac{AA^T}{n})$. Let $\widetilde{A} = \frac{AA^T}{n} = \frac{A}{n}$ and $p(z| \sigma^2)$ denotes the pdf of Gaussian distribution with zero mean and variance $\sigma^2$. We then have
\begin{equation*}
I_1 = \sum_{i=1}^n \int_z \xi^2(z) p(z| \widetilde{A}_{i}) dz
\end{equation*}
We use $p(x,y | \sigma^2_x, \sigma^2_y, \rho \sigma_x \sigma_y)$ to denote the pdf of two dimensional Gaussian distribution with zero mean. 
We further assume that every two rows of $A$ are linear independent.
We then have
\begin{equation*}
-I_2   = \sum_{i,j=1, i \neq j}^n A_{ij} \iint_{x,y} \xi(x)\xi(y) p(x,y| \widetilde{A}_{i}, \widetilde{A}_{j}, \widetilde{A}_{ij}) dx dy + \sum_{i=1}^n A_{ii} \int_z \xi^2(z) p(z | \widetilde{A}_i) dz 
\end{equation*}
Since we have $XX^T + \bar{X}\bar{X}^T = I_n$, $A_{ii} = \sum_{j=1}^k x^2_{ij} < 1$. Then we have
\begin{equation*}
I_1+ I_2 =   \sum_{i=1}^n (1-A_{ii}) \int_z \xi^2(z) p(z | \widetilde{A}_{ii}) dz  - \sum_{i,j=1, i \neq j}^n A_{ij} \iint_{x,y} \xi(x)\xi(y) p(x,y| \widetilde{A}_{ii}, \widetilde{A}_{jj}, \widetilde{A}_{ij}) dx dy
\end{equation*}
For $I_3$, the formula is quite tricky, we assume $\xi(z) = z^2 + a z + b $, then $\nabla \xi (AY) = \diag\{2AY+ a\}$. $[\cdot]_i$ represents the $i$-th element of a vector.
Then
\begin{equation*}
-I_3 = \sum_{i,j=1, i \neq j}^n A_{ij}\Sigma_{ij} + \sum_{i=1}^n A_{ii}\Sigma_{ii}
\end{equation*}
For $\Sigma_{ii}$ we have
\begin{equation*}
\Sigma_{ii} = \int_y [(I - A)Y]^2_i (2[AY]_i + a)^2 dy
\end{equation*}
The above integral is about $y_1, \dots, y_n$, which is a quadratic function about $a$.
For $\Sigma_{ij}$ with $i\neq j$ we have
\begin{equation*}
\Sigma_{ij} = \int_y [(I - A)Y]_i (2[AY]_i + a)[(I - A)Y]_j (2[AY]_j + a) dy
\end{equation*}
which is also a quadratic function about $a$.
For $I_1 + I_2$, the coefficient of $ab$ is zero and the other terms are all quandratic about $a$ and $b$ respectively. Therefore, we can find optimal $a$ and $b$ such that $I_1 + I_2 + I_3$ takes the maximum value.

The coefficient of $b^2, b$ takes the following form respectively:
\begin{align*}
\textrm{Coeff}(b^2) & = \sum_{i=1}^n (1-A_{ii}) - \sum_{i,j=1, i\neq j} A_{ij} \\
\textrm{Coeff}(b) & = \frac{2}{n}\sum_{i=1}^n A_{ii} [ 1 - A_{ii} - \sum_{j=1, j\neq i} A_{ij} ]
\end{align*}
The expectation about $\textrm{Coeff}(b^2)$ is positive. If  we assume
$$
E[A_{ij}]= \begin{cases}
\frac{k}{n} & i = j\\
0 & i\neq j 
\end{cases}
$$
Then $\textrm{Coeff}(b^2)  = n-k$. 
Therefore we can take apprropriate $b$ such that $\min \textrm{Coeff}(\epsilon^2)$ is positive.
It can be shown the coefficient of $a^2$ in $\textrm{Coeff}(a^2)$ is 
\begin{equation}
\textrm{Coeff}(a^2) = -\frac{1}{n}( \sum_{i,j=1}^n (n-2) A_{ij} + \sum_{i=1}^n A_{ii})
\end{equation}
The expectation of $\textrm{Coeff}(a^2)$ is negative. Therefore, we can choose $a$ such that $\textrm{Coeff}(\epsilon^2)$ is negative.

We conclude that the sign of $\min \textrm{Coeff}(\epsilon^2)$ is undetermined in the case of quadratic linear function pertubation.
\end{document}